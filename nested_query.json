Perfect. Letâ€™s assume for now that **Segment is still required**, and you're going to:

> Generate SAR mailing files (one per Contract + PBP + Segment), summary files, and error files â€” in tab-delimited CSV format â€” using Teradata as source.

---

# âœ… **Step-by-Step Plan to Generate SAR Files**

This plan includes:

* SQL logic in Teradata
* Output splitting and export
* Summary and error file generation
* File naming conventions
* Optional scheduling

---

## ðŸ”¹ STEP 1: **Create Required Lookup Tables (Temporary)**

Run this in Teradata first:

```sql
-- Drop and create SAR Plan inclusion list
CREATE MULTISET VOLATILE TABLE VT_SAR_PLAN (...) ON COMMIT PRESERVE ROWS;
INSERT INTO VT_SAR_PLAN VALUES (...);  -- Your SAR county list

-- Drop and create BOM mapping
CREATE MULTISET VOLATILE TABLE VT_SAR_NR_BOM_2026 (...) ON COMMIT PRESERVE ROWS;
INSERT INTO VT_SAR_NR_BOM_2026 VALUES (...);  -- Your Material/Replacement mapping
```

---

## ðŸ”¹ STEP 2: **Run Final SAR Detail SQL in Teradata**

This is the fully built query:

* Applies all SAR filters (contract, PBP, county)
* Derives `Segment` using span/transaction logic
* Joins with `VT_SAR_PLAN` and `VT_SAR_NR_BOM_2026`
* Outputs **one row per eligible member**

ðŸ’¡ **Export this queryâ€™s result to CSV file** (you can use BTEQ, TPT, or Teradata Studio).

Save as:

```
full_sar_output.csv
```

---

## ðŸ”¹ STEP 3: **Load Output into Pandas (Python)**

```python
import pandas as pd
from datetime import datetime

df = pd.read_csv("full_sar_output.csv")

# Normalize CSegment
df['CSegment'] = df['CSegment'].fillna('000').replace('', '000')
today_str = datetime.today().strftime('%Y%m%d')
```

---

## ðŸ”¹ STEP 4: **Split into One File per Contract + PBP + Segment**

```python
for (contract, pbp, segment), group in df.groupby(['CContract', 'CPBP', 'CSegment']):
    filename = f"2026SAR_Mailing_Fulfillment_{contract}_{pbp}_{segment}_{today_str}.txt"
    group.to_csv(filename, sep="\t", index=False)
```

âœ… Output: one file per Contract + PBP + Segment

---

## ðŸ”¹ STEP 5: **Create Summary File**

```python
summary_df = df.groupby(['CContract', 'CPBP', 'CSegment']).size().reset_index(name='MemberCount')
summary_filename = f"2026SAR_Mailing_Summary_{today_str}.txt"
summary_df.to_csv(summary_filename, sep="\t", index=False)
```

---

## ðŸ”¹ STEP 6: **Create Error File for Missing Required Fields**

```python
required_cols = [
    'Member ID', 'FirstName', 'LastName', 'Addr1', 'City', 'State', 'Zip',
    'CContract', 'CPBP', 'CSegment', 'Plan Name', 'LanguageText',
    'Plan State', 'Record ID', 'Record Type', 'Material ID', 'Plan Replacement ID'
]

error_df = df[df[required_cols].isnull().any(axis=1)]

if not error_df.empty:
    error_filename = f"2026SAR_Mailing_Fulfillment_error_{today_str}.txt"
    error_df.to_csv(error_filename, sep="\t", index=False)
```

---

## ðŸ”¹ STEP 7: **Daily Run Strategy (Optional)**

You can automate this:

* **Databricks notebook**: SQL + file generation in one flow
* **Airflow DAG**: Teradata extraction + Python tasks
* **Scheduled Python script**: via cron or Windows Task Scheduler

---

## ðŸ”¹ STEP 8: **File Delivery (Optional)**

* Send to vendor SFTP
* Upload to Azure Blob or AWS S3
* Email alerts for errors

---

## âœ… Files Generated Per Run

| File Type       | Format               | File Naming Example                                      |
| --------------- | -------------------- | -------------------------------------------------------- |
| SAR Detail File | .txt (tab-delimited) | `2026SAR_Mailing_Fulfillment_H4513_003_000_20250715.txt` |
| Summary File    | .txt                 | `2026SAR_Mailing_Summary_20250715.txt`                   |
| Error File      | .txt                 | `2026SAR_Mailing_Fulfillment_error_20250715.txt`         |

---

Let me know:

* If you want me to generate the full Python script from these steps
* If you want this adapted for `Contract + PBP` only (no Segment) later
* Or want help generating the BTEQ export script from Teradata
