In our MAA Databricks Platform, we have provisioned the Business Mart Service principal (and our own internal team) with access to create schemas in the gedp_{env} catalog. They need this access to support their deployment and testing process.Â 

However, provisioning this access now allows for the possibilities of junk schemas being created and not properly deleted. As such, in order to ensure that this is properly governed, we need to create a databricks job and accompanying alert that will run periodically to check the gedp_{env} catalog for the existence of any unexpected schemas.

This story will capture the analysis portion of this work, and another story wil be created to track the development work.

I had discussion with Cody on the design

We will create a databricks job in gedp-databricks-infra repo that gets passed the output of the terraform list of expected schemas as a parameter 

Example Databricks Job Terraform code to use as reference: https://github.com/zilvertonz/gedp-databricks-infra/blob/develop/module/databricks/ws_admin/dbx_job_enable_compute.tf
We can run the job daily to flag any schemas not expected and sent to 

gedp_workspace_admin_member_emails
