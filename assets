Databricks provides a wide range of **native connectors**, **libraries**, and **partner integrations** to ingest data from various source systems. These connectors cover structured, semi-structured, unstructured, streaming, and on-prem/cloud-based systems.

Hereâ€™s a categorized list of **common Databricks connectors for data ingestion**:

---

### ðŸ”¹ **Cloud Storage Connectors**

* **AWS S3**
* **Azure Data Lake Storage (ADLS) Gen1 & Gen2**
* **Google Cloud Storage (GCS)**
* **Blob Storage (Azure)**
* **Wasabi (via S3-compatible APIs)**

---

### ðŸ”¹ **Databases (JDBC/ODBC and Native Connectors)**

Databricks can connect to most databases via JDBC, and some have optimized native connectors:

* **MySQL, PostgreSQL**
* **SQL Server / Azure SQL DB**
* **Oracle**
* **Snowflake**
* **Google BigQuery**
* **Amazon Redshift**
* **Databricks Lakehouse Federation (Unity Catalog with External Tables)**

---

### ðŸ”¹ **Delta Sharing Connectors**

* **Delta Sharing (open protocol)**
* **Databricks-to-Databricks sharing**
* **Connect to Delta Shares via Tableau, Power BI, Python, or other Delta Sharing clients**

---

### ðŸ”¹ **Streaming Sources (Structured Streaming)**

* **Kafka / Confluent Kafka**
* **Event Hubs (Azure)**
* **AWS Kinesis**
* **Files from cloud storage with Autoloader (incremental file ingestion)**
* **Socket / TCP Streaming**
* **Pub/Sub (via connectors or Spark integrations)**

---

### ðŸ”¹ **ETL & Integration Tools**

* **Fivetran**
* **Matillion**
* **Informatica**
* **Qlik Replicate**
* **Talend**
* **dbt (for transformations)**

---

### ðŸ”¹ **SaaS Applications (via Partners or APIs)**

Usually via Fivetran, custom APIs, or partner connectors:

* **Salesforce**
* **Google Analytics / GA4**
* **Marketo, HubSpot**
* **Zendesk, ServiceNow**
* **NetSuite, Workday**
* **Shopify**

---

### ðŸ”¹ **File Formats**

Databricks supports a wide range of file formats:

* **CSV, JSON, Avro, Parquet, ORC**
* **Delta Lake**
* **XML (via libraries)**
* **Excel (via `com.crealytics:spark-excel` or Python/pandas)**

---

### ðŸ”¹ **Custom Ingestion Methods**

* **REST APIs** (via Python, Scala, or notebooks)
* **FTP/SFTP** (via Python or custom scripts)
* **SMB/NAS** (via mounted volumes or custom solutions)
* **Message brokers** (via Spark or Kafka integration)

---

### ðŸ”¹ **Data Integration via Unity Catalog (Federated Queries)**

* **External Tables from Google BigQuery, Snowflake, Redshift, Azure Synapse**
  (accessed via **Lakehouse Federation** or **Partner Connect**)

---

### ðŸ”¹ **Partner Connect Integrations**

Databricks provides 1-click integrations with:

* **Airbyte**
* **Fivetran**
* **Qlik**
* **dbt Cloud**
* **Power BI**
* **Tableau**
* **Sigma Computing**
* **Rivery**

---

Let me know if you want a table or diagram to summarize this, or need a focused list by use case (e.g., ingestion from SAP, mainframes, legacy files, etc.).
