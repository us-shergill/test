module "check_schemas" {
  source        = "git::https://github.com/zilvertonz/gedp-databricks-terraform-jobs.git?ref=main"
  env           = var.env
  domain_name   = var.domain_name
  project_name  = var.project_name
  spark_version = var.spark_version

  jobs = [
    {
      job_name     = "gedp_check_schemas_${var.env}"
      cluster_size = "xs"
      tasks = [
        {
          task_name   = "check_schemas_task"
          task_source = "${path.module}/artifacts/notebooks/check_schemas.py"
          task_type   = "NEW_NOTEBOOK"
          task_params = {
            env         = var.env
            expected_schemas = var.expected_schemas_list  # List of expected schemas passed to the job
          }
        }
      ]
    }
  ]

  schedule = {
    cron_expression = "0 0 21 ? * * *"  # Run daily at 9 PM New York time
    timezone_id     = "America/New_York"
    pause_status    = var.env == "prod" ? "UNPAUSED" : "PAUSED"
  }

  email_notifications = {
    on_start   = []
    on_success = local.gedp_workspace_admin_member_emails
    on_failure = local.gedp_workspace_admin_member_emails
  }
}





from pyspark.sql import SparkSession
import smtplib
import os

def get_existing_schemas(catalog):
    spark = SparkSession.builder.getOrCreate()
    query = f"SHOW SCHEMAS IN {catalog}"
    return [row.schemaName for row in spark.sql(query).collect()]

def send_alert(unexpected_schemas, emails):
    alert_message = f"Unexpected schemas detected: {', '.join(unexpected_schemas)}"
    # Sending email logic here (SMTP or any alerting tool in use)
    print(alert_message)

# Parameters (passed by the Databricks job)
catalog = f"gedp_{os.getenv('ENV')}"
expected_schemas = os.getenv('EXPECTED_SCHEMAS').split(',')

# Get existing schemas
existing_schemas = get_existing_schemas(catalog)

# Check for unexpected schemas
unexpected_schemas = set(existing_schemas) - set(expected_schemas)

if unexpected_schemas:
    send_alert(unexpected_schemas, os.getenv('GEDP_WORKSPACE_ADMIN_EMAILS').split(','))
